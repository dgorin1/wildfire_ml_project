{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a995d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file 'feds_western_us_2013_af_postprocessed.parquet'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/geopandas/io/arrow.py:650\u001b[39m, in \u001b[36m_read_parquet_schema_and_metadata\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m     schema = \u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.schema\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py:1424\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1421\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1422\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1424\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1426\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1427\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/dataset.py:790\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/dataset.py:472\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    471\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    475\u001b[39m     partitioning=partitioning,\n\u001b[32m    476\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    477\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    478\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    479\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/dataset.py:437\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: feds_western_us_2013_af_postprocessed.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[34;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Read in data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeds_western_us_2013_af_postprocessed.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCOLUMNS:\u001b[39m\u001b[33m\"\u001b[39m, df.columns)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCRS:\u001b[39m\u001b[33m\"\u001b[39m, df.crs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/geopandas/io/arrow.py:760\u001b[39m, in \u001b[36m_read_parquet\u001b[39m\u001b[34m(path, columns, storage_options, bbox, to_pandas_kwargs, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m filesystem, path = _get_filesystem_path(\n\u001b[32m    757\u001b[39m     path, filesystem=filesystem, storage_options=storage_options\n\u001b[32m    758\u001b[39m )\n\u001b[32m    759\u001b[39m path = _expand_user(path)\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m schema, metadata = \u001b[43m_read_parquet_schema_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    762\u001b[39m geo_metadata = _validate_and_decode_metadata(metadata)\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(geo_metadata[\u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m]) == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/geopandas/io/arrow.py:652\u001b[39m, in \u001b[36m_read_parquet_schema_and_metadata\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    650\u001b[39m     schema = parquet.ParquetDataset(path, filesystem=filesystem, **kwargs).schema\n\u001b[32m    651\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m652\u001b[39m     schema = \u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m metadata = schema.metadata\n\u001b[32m    656\u001b[39m \u001b[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001b[39;00m\n\u001b[32m    657\u001b[39m \u001b[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001b[39;00m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/parquet/core.py:2390\u001b[39m, in \u001b[36mread_schema\u001b[39m\u001b[34m(where, memory_map, decryption_properties, filesystem)\u001b[39m\n\u001b[32m   2388\u001b[39m file_ctx = nullcontext()\n\u001b[32m   2389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2390\u001b[39m     file_ctx = where = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx:\n\u001b[32m   2393\u001b[39m     file = ParquetFile(\n\u001b[32m   2394\u001b[39m         where, memory_map=memory_map,\n\u001b[32m   2395\u001b[39m         decryption_properties=decryption_properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/_fs.pyx:815\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_input_file\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Failed to open local file 'feds_western_us_2013_af_postprocessed.parquet'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Read in data\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"..\") / \"data\" / \"feds_western_us_2013_af_postprocessed.parquet\"\n",
    "\n",
    "df = gpd.read_parquet(data_path)\n",
    "\n",
    "print(\"COLUMNS:\", df.columns)\n",
    "print(\"CRS:\", df.crs)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba3efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the longest duration fire and subset it for initial pipeline development\n",
    "df_flat = df.reset_index()\n",
    "max_area = df_flat.farea.idxmax()\n",
    "max_fire_id = df_flat.loc[max_area,\"fireID\"]\n",
    "hero_fire = df_flat.loc[df_flat.fireID == max_fire_id,:]\n",
    "hero_fire.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3255a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geocube.api.core import make_geocube\n",
    "hero_fire = hero_fire.rename_geometry(\"geometry\")\n",
    "\n",
    "# 2. Now run the exact same code as before\n",
    "out_grid = make_geocube(\n",
    "    vector_data=hero_fire,\n",
    "    resolution=(-500, 500),\n",
    "    measurements=[\"fireID\"],\n",
    "    output_crs=\"EPSG:9311\",\n",
    "    group_by=\"t\"\n",
    ")\n",
    "\n",
    "print(out_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b0d9b0",
   "metadata": {},
   "source": [
    "## Okay, now I've got a simple grid for a single fire.. time to get weather data for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c556a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the tight bounds of the fire (minx, miny, maxx, maxy)\n",
    "minx, miny, maxx, maxy = hero_fire.total_bounds\n",
    "\n",
    "# 2. Define a buffer (e.g., 10km = 10,000 meters)\n",
    "BUFFER = 10000\n",
    "\n",
    "# 3. Create the \"Weather Box\"\n",
    "weather_box = [\n",
    "    minx - BUFFER,  # West edge moves further West\n",
    "    miny - BUFFER,  # South edge moves further South\n",
    "    maxx + BUFFER,  # East edge moves further East\n",
    "    maxy + BUFFER   # North edge moves further North\n",
    "]\n",
    "\n",
    "print(f\"Fire Bounds:    {hero_fire.total_bounds}\")\n",
    "print(f\"Weather Bounds: {weather_box}\")\n",
    "\n",
    "# 4. Define the Time Window\n",
    "start_time = hero_fire.t.min()\n",
    "end_time = hero_fire.t.max()\n",
    "\n",
    "print(f\"Need Weather from: {start_time} to {end_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628d22d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
